{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1bd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'gsutil' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r gs://cloud-ai-platform-c541b3e3-934f-414e-9196-8e2bf7a7fb59 .\n",
    "\n",
    "# cloud-ai-platform-c541b3e3-934f-414e-9196-8e2bf7a7fb59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7cd690",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m aiplatform\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexport_model_sample\u001b[39m(\n\u001b[0;32m      5\u001b[0m     project: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m      6\u001b[0m     model_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m ):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# The AI Platform services require regional API endpoints.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     client_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: api_endpoint}\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def export_model_sample(\n",
    "    project: str,\n",
    "    model_id: str,\n",
    "    gcs_destination_output_uri_prefix: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "    timeout: int = 300,\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n",
    "    output_config = {\n",
    "        \"artifact_destination\": {\n",
    "            \"output_uri_prefix\": gcs_destination_output_uri_prefix\n",
    "        },\n",
    "        # For information about export formats: https://cloud.google.com/ai-platform-unified/docs/export/export-edge-model#aiplatform_export_model_sample-drest\n",
    "        \"export_format_id\": \"tf-saved-model\",\n",
    "    }\n",
    "    name = client.model_path(project=project, location=location, model=model_id)\n",
    "    response = client.export_model(name=name, output_config=output_config)\n",
    "    print(\"Long running operation:\", response.operation.name)\n",
    "    print(\"output_info:\", response.metadata.output_info)\n",
    "    export_model_response = response.result(timeout=timeout)\n",
    "    print(\"export_model_response:\", export_model_response)\n",
    "\n",
    "export_model_sample(\n",
    "    project=\"llm-db\",\n",
    "    model_id=\"hf_11_24_smote\",\n",
    "    gcs_destination_output_uri_prefix=\"gs://cloud-ai-platform-c541b3e3-934f-414e-9196-8e2bf7a7fb59/exported_model\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa680abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio   # must come before loading the model\n",
    "\n",
    "# path to the directory containing saved_model.pb\n",
    "MODEL_DIR = \"./download_dir/.../predict/001\"\n",
    "\n",
    "# load the model\n",
    "loaded = tf.saved_model.load(MODEL_DIR)\n",
    "infer = loaded.signatures[\"serving_default\"]\n",
    "\n",
    "# read your test set\n",
    "df_test = pd.read_csv(\"path/to/your/test.csv\")\n",
    "features = [\"task_group\",\"author_category\",\"language_category\",\"location\"]  # whatever your model signature expects\n",
    "\n",
    "# build inputs dict\n",
    "inputs = {}\n",
    "for col in features:\n",
    "    # if your model expects string tensors:\n",
    "    inputs[col] = tf.constant(df_test[col].astype(str).values)\n",
    "    # or float: tf.constant(df_test[col].values, dtype=tf.float32)\n",
    "\n",
    "# call the model\n",
    "outputs = infer(**inputs)\n",
    "\n",
    "# inspect output keys\n",
    "print(\"Available outputs:\", list(outputs.keys()))\n",
    "# typically something like 'output_0' or 'predictions'\n",
    "pred_tensor = outputs[\"output_0\"]    # replace with the right key\n",
    "\n",
    "# if it’s logits or probabilities, you might need argmax\n",
    "if pred_tensor.shape[-1] > 1:\n",
    "    pred_indices = tf.argmax(pred_tensor, axis=-1).numpy()\n",
    "else:\n",
    "    # if binary, maybe threshold\n",
    "    pred_indices = (pred_tensor.numpy() > 0.5).astype(int).flatten()\n",
    "\n",
    "# decode indices back to category names if you kept a LabelEncoder\n",
    "# (assuming you saved it when exporting)\n",
    "#    from sklearn.preprocessing import LabelEncoder\n",
    "#    le = load_my_label_encoder()\n",
    "#    preds = le.inverse_transform(pred_indices)\n",
    "\n",
    "# attach predictions\n",
    "df_test[\"pred_idx\"] = pred_indices\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c79c1b",
   "metadata": {},
   "source": [
    "# Get predictions from server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8364ea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1098260 entries, 0 to 1098259\n",
      "Data columns (total 26 columns):\n",
      " #   Column               Non-Null Count    Dtype \n",
      "---  ------               --------------    ----- \n",
      " 0   model_id             1098260 non-null  object\n",
      " 1   num_downloads        1098260 non-null  int64 \n",
      " 2   num_likes            1098260 non-null  int64 \n",
      " 3   is_private           1098260 non-null  bool  \n",
      " 4   task                 1098260 non-null  object\n",
      " 5   tags                 1098260 non-null  object\n",
      " 6   author               1098260 non-null  object\n",
      " 7   author_category      1098260 non-null  object\n",
      " 8   base_model_relation  322 non-null      object\n",
      " 9   base_model           269044 non-null   object\n",
      " 10  language             1098260 non-null  object\n",
      " 11  model_creator        6528 non-null     object\n",
      " 12  model_type           4577 non-null     object\n",
      " 13  model_name           6433 non-null     object\n",
      " 14  model_card_tags      376584 non-null   object\n",
      " 15  datasets             86977 non-null    object\n",
      " 16  library_name         284726 non-null   object\n",
      " 17  task_group           1098260 non-null  object\n",
      " 18  language_category    1098260 non-null  object\n",
      " 19  downloads_category   1098260 non-null  object\n",
      " 20  base_model_category  269042 non-null   object\n",
      " 21  mixtral_response     269044 non-null   object\n",
      " 22  base_model_params    269042 non-null   object\n",
      " 23  model_type_cleaned   4569 non-null     object\n",
      " 24  first_commit_date    1070207 non-null  object\n",
      " 25  location             1098260 non-null  object\n",
      "dtypes: bool(1), int64(2), object(23)\n",
      "memory usage: 210.5+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/hf_models_withmodelcard_nov2024.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7fef265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_model_relation\n",
       "\"quantized\"    180\n",
       "\"adapter\"       53\n",
       "\"finetune\"      49\n",
       "\"merge\"         40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.base_model_relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "046f7892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24070"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.base_model.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b14b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_relation , base_model , model_card_tags , datasets, library_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87050ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1098260 entries, 0 to 1098259\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count    Dtype \n",
      "---  ------              --------------    ----- \n",
      " 0   task_group          1098260 non-null  object\n",
      " 1   author_category     1098260 non-null  object\n",
      " 2   language_category   1098260 non-null  object\n",
      " 3   location            1098260 non-null  object\n",
      " 4   downloads_category  1098260 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 41.9+ MB\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    \"task_group\",\n",
    "    \"author_category\",\n",
    "    \"language_category\",\n",
    "    \"location\",\n",
    "    \"downloads_category\"\n",
    "]\n",
    "df = df[features]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e925322a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_group</th>\n",
       "      <th>author_category</th>\n",
       "      <th>language_category</th>\n",
       "      <th>location</th>\n",
       "      <th>downloads_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text Processing</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>High</td>\n",
       "      <td>us</td>\n",
       "      <td>Very Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>High</td>\n",
       "      <td>us</td>\n",
       "      <td>Very Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>High</td>\n",
       "      <td>us</td>\n",
       "      <td>Very Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Silver</td>\n",
       "      <td>High</td>\n",
       "      <td>us</td>\n",
       "      <td>Very Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Silver</td>\n",
       "      <td>High</td>\n",
       "      <td>us</td>\n",
       "      <td>Very Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        task_group author_category language_category location  \\\n",
       "0  Text Processing          Bronze              High       us   \n",
       "1          Unknown          Bronze              High       us   \n",
       "2          Unknown          Bronze              High       us   \n",
       "3          Unknown          Silver              High       us   \n",
       "4          Unknown          Silver              High       us   \n",
       "\n",
       "  downloads_category  \n",
       "0           Very Low  \n",
       "1           Very Low  \n",
       "2           Very Low  \n",
       "3           Very Low  \n",
       "4           Very Low  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0be66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "downloads_category\n",
       "Very Low    807979\n",
       "Low         249325\n",
       "Mid          26888\n",
       "High         14068\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.downloads_category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "106222ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1098260 instances to request.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "sample = df\n",
    "\n",
    "# 2) For each row, build a dict of your feature columns\n",
    "instances = []\n",
    "for _, row in sample.iterrows():\n",
    "    instances.append({\n",
    "        \"task_group\": row[\"task_group\"],\n",
    "        \"author_category\": row[\"author_category\"],\n",
    "        \"language_category\": row[\"language_category\"],\n",
    "        \"location\": row[\"location\"]\n",
    "    })\n",
    "\n",
    "payload = {\"instances\": instances}\n",
    "\n",
    "# 3) Write that to a JSON file\n",
    "with open(\"request.json\", \"w\") as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "print(\"Wrote\", len(instances), \"instances to request.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "773165c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 109826 instances to request_part1.json\n",
      "Wrote 109826 instances to request_part2.json\n",
      "Wrote 109826 instances to request_part3.json\n",
      "Wrote 109826 instances to request_part4.json\n",
      "Wrote 109826 instances to request_part5.json\n",
      "Wrote 109826 instances to request_part6.json\n",
      "Wrote 109826 instances to request_part7.json\n",
      "Wrote 109826 instances to request_part8.json\n",
      "Wrote 109826 instances to request_part9.json\n",
      "Wrote 109826 instances to request_part10.json\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Split the instances list into 10 roughly equal parts\n",
    "num_parts = 10\n",
    "part_size = math.ceil(len(instances) / num_parts)\n",
    "for i in range(num_parts):\n",
    "    start = i * part_size\n",
    "    end = min((i + 1) * part_size, len(instances))\n",
    "    part = instances[start:end]\n",
    "    filename = f\"request_part{i+1}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump({\"instances\": part}, f, indent=2)\n",
    "    print(f\"Wrote {len(part)} instances to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b5f19c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [{\"scores\": [0.3295256793498993, 0.36474868655204773, 0.14926689863204956, 0.15645872056484222], \"classes\": [\"High\", \"Mid\", \"Very Low\", \"Low\"]}, {\"scores\": [0.3295256793498993, 0.36474868655204773, 0.14926689863204956, 0.15645872056484222], \"classes\": [\"High\", \"Mid\", \"Very Low\", \"Low\"]}, {\"scores\": [0.3295256793498993, 0.36474868655204773, 0.14926689863204956, 0.15645872056484222], \"classes\": [\"High\", \"Mid\", \"Very Low\", \"Low\"]}, {\"scores\": [0.3295256793498993, 0.36474868655204773, 0.14926689863204956, 0.15645872056484222], \"classes\": [\"High\", \"Mid\", \"Very Low\", \"Low\"]}, {\"scores\": [0.3295256793498993, 0.36474868655204773, 0.14926689863204956, 0.15645872056484222], \"classes\": [\"High\", \"Mid\", \"Very Low\", \"Low\"]}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1420  100   737  100   683   3011   2790 --:--:-- --:--:-- --:--:--  5795\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST -H \"Content-Type: application/json\" --data @request.json http://localhost:8080/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e93ffdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load your full test set\n",
    "df_test = pd.read_csv(\"../data/hf_models_withmodelcard_nov2024.csv\")\n",
    "X_test = df_test[[\"task_group\",\"author_category\",\"language_category\",\"location\"]]\n",
    "y_true = df_test[\"downloads_category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0058eb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "downloads_category\n",
       "Very Low    807979\n",
       "Low         249325\n",
       "Mid          26888\n",
       "High         14068\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.downloads_category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5045e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'task_group': 'Text Processing', 'author_category': 'Bronze', 'language_category': 'High', 'location': 'us'}, {'task_group': 'Unknown', 'author_category': 'Bronze', 'language_category': 'High', 'location': 'us'}, {'task_group': 'Unknown', 'author_category': 'Bronze', 'language_category': 'High', 'location': 'us'}, {'task_group': 'Unknown', 'author_category': 'Silver', 'language_category': 'High', 'location': 'us'}, {'task_group': 'Unknown', 'author_category': 'Silver', 'language_category': 'High', 'location': 'us'}]\n"
     ]
    }
   ],
   "source": [
    "instances = X_test.to_dict(orient=\"records\")\n",
    "print(instances[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43eaf45a",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: http://localhost:8080/predict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:8080/predict\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      2\u001b[0m                   json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m: instances})\n\u001b[1;32m----> 3\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m resp \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# pick the top‑scoring class for each\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sahil\\miniconda3\\envs\\potnet\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: http://localhost:8080/predict"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "r = requests.post(\"http://localhost:8080/predict\",\n",
    "                  json={\"instances\": instances})\n",
    "r.raise_for_status()\n",
    "resp = r.json()\n",
    "\n",
    "# pick the top‑scoring class for each\n",
    "y_pred = []\n",
    "for pred in resp[\"predictions\"]:\n",
    "    # assume classes list is the same for every row:\n",
    "    classes = pred[\"classes\"]\n",
    "    best = pred[\"scores\"].index(max(pred[\"scores\"]))\n",
    "    y_pred.append(classes[best])\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971b890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "potnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
